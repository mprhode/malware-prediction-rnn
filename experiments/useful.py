import os.path
import numpy as np
import keras.backend as K
from keras import callbacks

np.random.seed(11)

def unison_shuffled_copies(arrays):
	"""shuffle multiple arrays keeping corresponding indicies in the same way"""
	length = len(arrays[1])
	for x in arrays:
		assert len(x) == length
	p = np.random.permutation(length)
	return tuple([x[p] for x in arrays])

def get_mean_and_stdv(dataset):
	"""return means and standard deviations along 0th axis of tensor"""
	means = dataset.mean(0)
	stdvs = dataset.std(0)
	return means, stdvs

def scale_array(dataset, means, stdvs):
	"""normalise by means and standard deviations along 0th axis of tensor"""
	return (dataset - means) / (stdvs + K.epsilon())

def check_filename(originalName):
	""" add number to filename to make unique if not already - returns new name"""
	count = 1
	exists = True
	fileName = originalName
	while(exists):
		exists = os.path.exists(fileName)
		if exists:
			if "." in originalName:
				fileName = originalName.split(".")
				fileName[0] += "_" + str(count)
				fileName = ".".join(fileName)
			else: 
				fileName = originalName + "_" + str(count)
			count += 1
	return fileName

def truncate_and_tensor(dataX, dataY, length):
	"""crop any longer than length, return new X and Y tensors"""
	new_X = []
	new_Y = []

	for x in list(range(len(dataX))):
		new_X.append(dataX[x][:length])
		new_Y.append(dataY[x][:length])

	x, y = to_numpy_tensors(new_X, new_Y)
	return x, y

def to_numpy_tensors(x, y):
	""" return x and y as numpy tensors, x as a 3-D tensor, y as a 2-D tensor"""
	x_shape = (len(x), len(x[0]), len(x[0][0]))
	y_shape = (len(y), 1)
	
	if type(x) is list:
		x = np.array(x)
	if type(y) is list:
		y = np.array(y)

	x = x.reshape(x_shape)
	y = y.reshape(y_shape)

	return x, y

def remove_short(dataX, dataY, length):
	"""Remove any seqences in dataX shorter than length, crop any longer than length, return new X and Y tensors"""
	new_X = []
	new_Y = []

	for x in list(range(len(dataX))):
		if len(dataX[x]) >= length:
			new_X.append(dataX[x][:length])
			new_Y.append(dataY[x][:length])

	x, y = to_numpy_tensors(new_X, new_Y)
	return x, y

def remove_short_idx(dataX, dataY, idxs, length):
	new_X = []
	new_Y = []
	new_idxs = []
	
	for x in list(range(len(dataX))):
		if len(dataX[x]) >= length:
			new_X.append(dataX[x][:length])
			new_Y.append(dataY[x][:length])
			new_idxs.append(idxs[x])
	
	x,y = to_numpy_tensors(new_X, new_Y)
	return x, y, np.array(new_idxs)


def timestamped_to_vector(data, timestamp_col, time_start, classification_col):
	"""Return tuple of inputs and outputs)
	Removes timestamp - not desirable if irregular inputs
	"""
	x = []
	y = []
	temp_inputs = []
	temp_outputs = []
	for input_row in data:
		#print([np.round(x) for x in input_row])
		if (input_row[timestamp_col] == time_start):
			if (temp_inputs != [] and temp_outputs != []):
				x.append(temp_inputs)
				y.append(temp_outputs)
				temp_inputs = []
				temp_outputs = []
		keep = np.array([x for x in range(len(input_row - 1)) if x not in [classification_col, timestamp_col]])
		temp_inputs.append(input_row[keep])
		temp_outputs = [int(input_row[classification_col])]

	if (temp_inputs != [] and temp_outputs != []):
		x.append(temp_inputs)
		y.append(temp_outputs)

	x = np.asarray(x)
	y = np.asarray(y)
	return x, y

def array_to_list(arr):
	try:
		return arr.tolist()
	except AttributeError:
		return arr

def into_sliding_chunk_arrays(data_x, chunk_size):
	"""return array of data cut into chunks of size chunk_size sliding along array
	and array of tuples denoting starting index and end index for chunk"""
	big_X = []
	idx_len_tuples = []
	data_temp = array_to_list(data_x)
	for i in list(range(len(data_temp[0]) - chunk_size+1)):
		x = []
		for s, sequence in enumerate(data_temp):
			x.append(sequence[i:i+chunk_size])

		big_X.append(np.array(x))
		idx_len_tuples.append((i, chunk_size))
	return big_X, idx_len_tuples

def to_chunks(l, num_chunks):
	last_one = 0
	chunks = []
	n = len(l)//num_chunks

	#Most samples spread out
	for i in range(0, n*num_chunks, n):
		chunks.append(l[i:i + n])
	
	#Extras distributed evenly:
	for i in range(n*num_chunks, len(l)):
		chunks[last_one].append(l[i])
		last_one = (last_one + 1) % num_chunks
		
		return chunks


def merge_two_dicts(x, y):
    """Given two dicts, merge them into a new dict as a shallow copy. Credit: http://stackoverflow.com/questions/38987/how-to-merge-two-python-dictionaries-in-a-single-expression"""
    z = x.copy()
    z.update(y)
    return z

class ResetStatesCallback(callbacks.Callback):
	def __init__(self):
		self.current_epoch = 0

	def on_batch_begin(self, batch, logs={}):
		self.model.reset_states()